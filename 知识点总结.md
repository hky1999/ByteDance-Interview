## 预测智力题：

#### 1、用随机数012来生成012345

这个题的难点在于如何保证   数字出现的概率都是相等 的   **。**   

​    0-6通过对7取余可以得到，那么就想办法凑对7取余的场景。   

```java
public class Frequency {
    public static int rand7(){
        while(true){
            int num=5*rand5()+rand5();//0-24
            if(num<21)
                return num % 7;
        }
    }
}
//变形：如果用0-6随机生成器生成0-9随机数
public class Frequency {
    public static int rand10(){
        while(true){
            int num=7*rand7()+rand7();
            if(num<41)
            //排除41-48，因为他们不能生成9，会造成各个数字出现的概率不同
                return num % 10;
        }
    }
}
```

#### 2、海盗分金币

5个海盗抢到了100颗宝石，每一颗都一样的大小和价值连城。他们决定bai这么分：
1、抽签决定自己的号码（1，2，3，4，5）
2、首先，由1号提出分配方案，然后大家5人进行表决，当且仅当超过半数的人同意时，按照他的提案进行分配，否则他将被扔入大海喂鲨鱼。
3、如果1号死后，再由2号提出分配方案，然后大家4人进行表决，当且仅当超过半数的人同意时，按照他的提案进行分配，否则他将被扔入大海喂鲨鱼。
4、以此类推，直到最终得出一个分配方案。
条件：每个海盗都是很聪明的人，都能很理智的判断得失，从而做出选择。
问题：假如你是1号海盗，则你应该提出什么样的分配方案可以使自己的收益最大化（也就是在保命的前提下自己得到的宝石最多）？

#### 3、一个线段切成三段，构成三角形的概率

1/4

#### 4、从64匹马中找到最快的4匹马

11次

## Linux文件系统、零拷贝

https://www.cnblogs.com/rickiyang/p/13265043.html#3920716807

##### 文件读写基本流程

**读文件**

1. 进程调用库函数向内核发起读文件请求；
2. 内核通过检查进程的文件描述符定位到虚拟文件系统的已打开文件列表项；
3. 调用该文件可用的系统调用函数 `read()`；
4. `read()` 函数通过文件表项链接到目录项模块，根据传入的文件路径，在目录项模块中检索，找到该文件的 `inode`；
5. 在 `inode` 中，通过文件内容偏移量计算出要读取的页；
6. 通过 `inode` 找到文件对应的 `address_space`；
7. 在 `address_space` 中访问该文件的页缓存树，查找对应的页缓存结点：
   1. 如果页缓存命中，那么直接返回文件内容；
   2. 如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过`inode` 找到文件该页的磁盘地址，读取相应的页填充该缓存页；
   3. 重新进行第 6 步查找页缓存；
8. 文件内容读取成功。

总结一下：`inode` 管磁盘，`address_space` 接内存，两者互相指针链接。

`Inode` 是文件系统(VFS)下的概念，通过 **一个 inode 对应一个文件** 使得文件管理按照类似索引的这种树形结构进行管理，通过 `inode` 快速的找到文件在磁盘扇区的位置；但是这种管理机制并不能满足读写的要求，因为我们修改文件的时候是先修改内存里的，所以就有了页缓存机制，作为内存与文件的缓冲区。
`address_space` 模块表示一个文件在页缓存中已经缓存了的物理页。它是页缓存和外部设备中文件系统的桥梁。如果将文件系统可以理解成数据源，那么 `address_space` 可以说关联了内存系统和文件系统。

**写文件**

前5步和读文件一致，在 `address_space` 中查询对应页的页缓存是否存在；

1. 如果页缓存命中，直接把文件内容修改更新在页缓存的页中，写文件就结束了。这时候文件修改位于页缓存，并没有写回到磁盘文件中去。
2. 如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过 `inode` 找到文件该页的磁盘地址，读取相应的页填充该缓存页。此时缓存页命中，进行第 6 步。
3. 一个页缓存中的页如果被修改，那么会被标记成脏页，脏页需要写回到磁盘中的文件块。有两种方式可以把脏页写回磁盘：
   1. 手动调用 `sync()` 或者 `fsync()` 系统调用把脏页写回；
   2. pdflush 进程会定时把脏页写回到磁盘。

同时注意，脏页不能被置换出内存，如果脏页正在被写回，那么会被设置写回标记，这时候该页就被上锁，其他写请求被阻塞直到锁释放。

#### 传统I/O

##### 传统读操作

```cpp
read(file_fd, tmp_buf, len);
```

基于传统的 I/O 读取方式，read 系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝，发起数据读取的流程如下：

1. 用户进程通过`read()`函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态 (kernel space)；
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间 (kernel space) 的读缓冲区 (read buffer)；
3. CPU 将读缓冲区 (read buffer) 中的数据拷贝到用户空间 (user space) 的用户缓冲区 (user buffer)。
4. 上下文从内核态 (kernel space) 切换回用户态 (user space)，read 调用执行返回。

##### 传统写操作

当应用程序准备好数据，执行 write 系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区(socket buffer)中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。

```cpp
Copywrite(socket_fd, tmp_buf, len);
```

基于传统的 I/O 写入方式，`write()` 系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝，用户程序发送网络数据的流程如下：

1. 用户进程通过 `write()` 函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)。
2. CPU 将用户缓冲区 (user buffer) 中的数据拷贝到内核空间 (kernel space) 的网络缓冲区 (socket buffer)。
3. CPU 利用 DMA 控制器将数据从网络缓冲区 (socket buffer) 拷贝到网卡进行数据传输。
4. 上下文从内核态 (kernel space) 切换回用户态 (user space)，write 系统调用执行返回。

##### 零拷贝方式

在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。

- 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
- 减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。
- 写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。

##### 用户态直接 I/O

##### mmap + write

1. 用户进程通过 `mmap()` 函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)；
2. 将用户进程的内核空间的读缓冲区 (read buffer) 与用户空间的缓存区 (user buffer) 进行内存地址映射；
3. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间 (kernel space) 的读缓冲区 (read buffer)；
4. 上下文从内核态 (kernel space) 切换回用户态 (user space)，mmap 系统调用执行返回；
5. 用户进程通过`write()` 函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)；
6. CPU 将读缓冲区 (read buffer) 中的数据拷贝到的网络缓冲区 (socket buffer) ；
7. CPU 利用 DMA 控制器将数据从网络缓冲区 (socket buffer) 拷贝到网卡进行数据传输；
8. 上下文从内核态 (kernel space) 切换回用户态 (user space) ，write 系统调用执行返回；

##### sendfile

1. 用户进程通过 `sendfile()` 函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间 (kernel space) 的读缓冲区 (read buffer)。
3. CPU 将读缓冲区 (read buffer) 中的数据拷贝到的网络缓冲区 (socket buffer)。
4. CPU 利用 DMA 控制器将数据从网络缓冲区 (socket buffer) 拷贝到网卡进行数据传输。
5. 上下文从内核态 (kernel space) 切换回用户态 (user space)，sendfile 系统调用执行返回。

##### sendfile + DMA gather copy

1. 用户进程通过 `sendfile()`函数向内核 (kernel) 发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间 (kernel space) 的读缓冲区 (read buffer)。
3. CPU 把读缓冲区 (read buffer) 的文件描述符(file descriptor)和数据长度拷贝到网络缓冲区(socket buffer)。
4. 基于已拷贝的文件描述符 (file descriptor) 和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区 (read buffer) 拷贝到网卡进行数据传输。
5. 上下文从内核态 (kernel space) 切换回用户态 (user space)，sendfile 系统调用执行返回。

#### splice

1. 用户进程通过 `splice()` 函数向内核(kernel)发起系统调用，上下文从用户态 (user space) 切换为内核态(kernel space)；
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间 (kernel space) 的读缓冲区 (read buffer)；
3. CPU 在内核空间的读缓冲区 (read buffer) 和网络缓冲区(socket buffer)之间建立管道 (pipeline)；
4. CPU 利用 DMA 控制器将数据从网络缓冲区 (socket buffer) 拷贝到网卡进行数据传输；
5. 上下文从内核态 (kernel space) 切换回用户态 (user space)，splice 系统调用执行返回。

##### 写时复制  需要 MMU 的支持

##### 缓冲区共享

fbuf 的思想是每个进程都维护着一个缓冲区池，这个缓冲区池能被同时映射到用户空间 (user space) 和内核态 (kernel space)，内核和用户共享这个缓冲区池，这样就避免了一系列的拷贝操作。

#### Linux零拷贝对比

无论是传统 I/O 拷贝方式还是引入零拷贝的方式，2 次 DMA Copy 是都少不了的，因为两次 DMA 都是依赖硬件完成的。下面从 CPU 拷贝次数、DMA 拷贝次数以及系统调用几个方面总结一下上述几种 I/O 拷贝方式的差别。

| 拷贝方式                   | CPU拷贝 | DMA拷贝 |   系统调用   | 上下文切换 |
| -------------------------- | :-----: | :-----: | :----------: | :--------: |
| 传统方式(read + write)     |    2    |    2    | read / write |     4      |
| 内存映射(mmap + write)     |    1    |    2    | mmap / write |     4      |
| sendfile                   |    1    |    2    |   sendfile   |     2      |
| sendfile + DMA gather copy |    0    |    2    |   sendfile   |     2      |
| splice                     |    0    |    2    |    splice    |     2      |

### GIC中断控制器

对于GIC，它可以管理4种类型的中断：

（1）外设中断（Peripheral interrupt）。根据目标CPU的不同，外设的中断可以分成PPI（Private Peripheral Interrupt）和SPI（Shared Peripheral Interrupt）。PPI只能分配给一个确定的processor，而SPI可以由Distributor将中断分配给一组Processor中的一个进行处理。外设类型的中断一般通过一个interrupt request line的硬件信号线连接到中断控制器，可能是电平触发的（Level-sensitive），也可能是边缘触发的（Edge-triggered）。

（2）软件触发的中断（SGI，Software-generated interrupt）。软件可以通过写GICD_SGIR寄存器来触发一个中断事件，这样的中断，可以用于processor之间的通信。

（3）虚拟中断（Virtual interrupt）和Maintenance interrupt。

### C++

- 重载 早绑定

- 重写 晚绑定，与多态相关

- 全局对象的构造函数会在main 函数之前执行

- static inline的内联函数，一般情况下不会产生函数本身的代码，而是全部被嵌入在被调用的地方。

- 一个"static inline"函数促使编译程序尝试着将其代码插入到所有调用它的程序中。

- 使用inline会增加二进制映像的大小，而这会降低访问CPU高速缓存的速度，所以不能在所有的函数定义中使用它。

- 注意当数组作为函数的参数进行传递时，该数组自动退化为同类型的指针。

- 纯虚函数不能再在基类中实现，编译器要求在派生类中必须予以重写以实现多态性。

C++ STL实现：

- vector 是动态数组。在堆上分配空间。
- vector 动态增加大小，并不是在原空间之后持续新空间（因为无法保证原空间之后尚有可供配置的空间），而是以原大小的两倍另外配置一块较大的空间，然后将原内容拷贝过来，然后才开始在原内容之后构造新元素，并释放原空间。因此，对 vector 的任何操作，一旦引起空间重新配置，同时指向原vector 的所有迭代器就都失效了。
- STL 中的list 底层是一个双向链表，而且是一个环状双向链表。这个特点使得它的随即存取变的非常没有效率，因此它没有提供 [] 操作符的重载。
- deque 是一种双向开口的连续线性空间，元素也是在堆中。所谓双向开口，意思是可以在队尾两端分别做元素的插入和删除操作。deque 和 vector 的最大差异，一在于 deque 允许于常数时间内对起头端进行元素的插入或移除操作，二在于deque没有所谓容量观念，因为它是动态地以分段连续空间组合而成，随时可以增加一段新的空间并链接在一起。换句话说，像 vector 那样“因旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在 deque 是不会发生的。

C++实现临界区访问

- ​	win：CriticalSection 临界区
- ​	Linux： pthread 互斥量

http相关

时间复杂度

排序问题，数组

协程，并发

### 死锁的四个必要条件

- 互斥条件：一个资源每次只能被一个进程使用，即在一段时间内某 资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。
- 请求与保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源 已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。
- 不可剥夺条件: 进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能 由获得该资源的进程自己来释放（只能是主动释放)。
- 循环等待条件: 若干进程间形成首尾相接循环等待资源的关系
